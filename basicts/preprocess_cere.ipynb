{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-30T02:06:14.934624Z",
     "start_time": "2025-03-30T02:03:43.621426Z"
    }
   },
   "source": [
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# note 先利用pandas读取csv数据，过滤掉没什么用的ID\n",
    "# 需要注意的是，SME的Code是2\n",
    "cere_base_dir = \"/data/hanzhi/CER-E/\"\n",
    "allocation_file = os.path.join(cere_base_dir, \"SME and Residential allocations.csv\")\n",
    "df = pd.read_csv(allocation_file)\n",
    "ids = df.get(\"ID\")\n",
    "codes = df.get(\"Code\")\n",
    "sme_ids = []\n",
    "for i, c in zip(ids, codes):\n",
    "    if c == 2:\n",
    "        sme_ids.append(i)\n",
    "file_names = [f\"File{i}.txt\" for i in range(1, 7)]\n",
    "sme_ids = set(sme_ids)\n",
    "\n",
    "# note 依次读取files，过滤掉不属于上面的id的内容，得到{id: [(timestamp, value)]}\n",
    "sme_id2read = {i: [] for i in sme_ids}\n",
    "timestamps = set()\n",
    "for f in file_names:\n",
    "    print(f\"reading {f}\")\n",
    "    file = open(os.path.join(cere_base_dir, f))\n",
    "    for line in file.readlines():\n",
    "        items = line.split(\" \")\n",
    "        meter_id = int(items[0])\n",
    "        timestamp = int(items[1])\n",
    "        value = float(items[2])\n",
    "        timestamps.add(timestamp)\n",
    "        if meter_id not in sme_ids:\n",
    "            continue\n",
    "        # print(meter_id, timestamp, value)\n",
    "        sme_id2read[meter_id].append((timestamp, value))\n",
    "\n",
    "# note 再对上面的时间步进行排序，得到各个时间序列，到这一步应该有485条时间序列\n",
    "min_ts = min(timestamps)\n",
    "real_min_ts = (min_ts // 100) * 48 + (min_ts % 48 - 1)\n",
    "print(\"min ts\", min_ts, real_min_ts)\n",
    "\n",
    "# note 目标文件\n",
    "target_dir = \"/data3/hanzhi/BasicTS/datasets/CER-E\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# note 保存sme_id\n",
    "# 先对sme_id进行排序\n",
    "sme_ids = list(sme_ids)\n",
    "sme_ids.sort()\n",
    "np.savez(os.path.join(target_dir, \"sme_ids.npz\"), sme_ids=sme_ids)\n",
    "# note 保存timestamp\n",
    "timestamps = [real_min_ts + i for i in range(25728)]\n",
    "np.savez(os.path.join(target_dir, \"timestamps.npz\"), timestamps=timestamps)\n",
    "\n",
    "# 再保存\n",
    "# note 保存原始时间序列，称为data_raw.dat，只有一个元素，没有filling\n",
    "# 每一个都截断到只有前25728个\n",
    "# 目标数组\n",
    "arr = np.full(shape=(len(sme_id2read), 25728), fill_value=0., dtype=np.float32)\n",
    "truncated = 0\n",
    "\n",
    "for idx, sme_id in tqdm(enumerate(sme_ids)):\n",
    "    sme_id2read[sme_id].sort()\n",
    "    for ts, v in sme_id2read[sme_id]:\n",
    "        # 一个三位数，到2009年1月1日的距离\n",
    "        day_since_start = ts // 100\n",
    "        # 一个两位数，0~47，表明今天的第几个半小时\n",
    "        hour_of_day = ts % 100 -1\n",
    "        real_ts = day_since_start * 48 + hour_of_day\n",
    "        if real_ts - real_min_ts >= 25728:\n",
    "            truncated += 1\n",
    "        else:\n",
    "            arr[idx, real_ts - real_min_ts] = v\n",
    "print(f\"truncated {truncated}\", f\"null value {np.sum(arr[arr==0.0])}\")\n",
    "np.savez(os.path.join(target_dir, \"arr.npz\"), arr=arr)\n",
    "# note 将这些时间序列保存下来，暂时先不考虑空值的问题"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading File1.txt\n",
      "reading File2.txt\n",
      "reading File3.txt\n",
      "reading File4.txt\n",
      "reading File5.txt\n",
      "reading File6.txt\n",
      "min ts 19501 9372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "485it [00:06, 75.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truncated 0 null value 0.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T02:06:22.174443Z",
     "start_time": "2025-03-30T02:06:20.772751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# note 先把arr增加day_of_week和time_of_day\n",
    "target_dir = \"/data3/hanzhi/BasicTS/datasets/CER-E\"\n",
    "original_data = np.load(os.path.join(target_dir, \"arr.npz\"))['arr']\n",
    "timestamps = np.load(os.path.join(target_dir, \"timestamps.npz\"))['timestamps']\n",
    "print(\"original_data_shape\", original_data.shape, original_data.dtype, original_data[0,:48])\n",
    "count_nodes, count_timestamps = original_data.shape\n",
    "print(\"original_timestamp_shape\", timestamps.shape)\n",
    "dow = 2\n",
    "tod = 0\n",
    "dows = []\n",
    "tods = []\n",
    "for t in timestamps.tolist():\n",
    "    dows.append(dow)\n",
    "    tods.append(tod)\n",
    "    tod += 1\n",
    "    tod %= 48\n",
    "    if tod == 0:\n",
    "        dow += 1\n",
    "        dow %= 7\n",
    "# (count_timestamps) -> (1, count_timestamps) -> (count_nodes, timestamps)\n",
    "dows = np.tile(np.expand_dims(np.array(dows, dtype=np.float32) / 7, axis=0), (count_nodes, 1))\n",
    "tods = np.tile(np.expand_dims(np.array(tods, dtype=np.float32) / 48, axis=0), (count_nodes, 1))\n",
    "# 3*(count_nodes, count_timestamps) -> (count_nodes, count_timestamps, 3)\n",
    "new_data = np.stack((original_data, tods, dows), axis=-1)\n",
    "new_data = new_data.transpose((1, 0, 2))\n",
    "print(\"new data shape\", new_data.shape)\n",
    "written_data = np.memmap(os.path.join(target_dir, \"data.dat\"), mode=\"write\", shape=new_data.shape, dtype=new_data.dtype)\n",
    "print(new_data[:48, 0, :])\n",
    "new_data[new_data == -32.0] = 0.\n",
    "written_data[:] = new_data\n",
    "written_data.flush()\n",
    "print(np.sum(new_data[new_data == 0.0]))\n",
    "print(np.sum(new_data == 0.))"
   ],
   "id": "7ab6689dfdb82637",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_data_shape (485, 25728) float32 [0.016 0.016 0.009 0.008 0.072 0.138 0.704 0.539 0.698 0.757 0.671 0.683\n",
      " 0.676 0.553 0.757 0.768 0.588 0.633 0.999 0.879 0.813 0.681 0.63  0.778\n",
      " 0.755 0.6   0.719 0.74  1.154 0.957 0.804 0.032 0.018 0.018 0.018 0.018\n",
      " 0.018 0.017 0.017 0.017 0.017 0.018 0.018 0.018 0.018 0.018 0.018 0.018]\n",
      "original_timestamp_shape (25728,)\n",
      "new data shape (25728, 485, 3)\n",
      "[[0.016      0.         0.2857143 ]\n",
      " [0.016      0.02083333 0.2857143 ]\n",
      " [0.009      0.04166667 0.2857143 ]\n",
      " [0.008      0.0625     0.2857143 ]\n",
      " [0.072      0.08333334 0.2857143 ]\n",
      " [0.138      0.10416666 0.2857143 ]\n",
      " [0.704      0.125      0.2857143 ]\n",
      " [0.539      0.14583333 0.2857143 ]\n",
      " [0.698      0.16666667 0.2857143 ]\n",
      " [0.757      0.1875     0.2857143 ]\n",
      " [0.671      0.20833333 0.2857143 ]\n",
      " [0.683      0.22916667 0.2857143 ]\n",
      " [0.676      0.25       0.2857143 ]\n",
      " [0.553      0.27083334 0.2857143 ]\n",
      " [0.757      0.29166666 0.2857143 ]\n",
      " [0.768      0.3125     0.2857143 ]\n",
      " [0.588      0.33333334 0.2857143 ]\n",
      " [0.633      0.35416666 0.2857143 ]\n",
      " [0.999      0.375      0.2857143 ]\n",
      " [0.879      0.39583334 0.2857143 ]\n",
      " [0.813      0.41666666 0.2857143 ]\n",
      " [0.681      0.4375     0.2857143 ]\n",
      " [0.63       0.45833334 0.2857143 ]\n",
      " [0.778      0.47916666 0.2857143 ]\n",
      " [0.755      0.5        0.2857143 ]\n",
      " [0.6        0.5208333  0.2857143 ]\n",
      " [0.719      0.5416667  0.2857143 ]\n",
      " [0.74       0.5625     0.2857143 ]\n",
      " [1.154      0.5833333  0.2857143 ]\n",
      " [0.957      0.6041667  0.2857143 ]\n",
      " [0.804      0.625      0.2857143 ]\n",
      " [0.032      0.6458333  0.2857143 ]\n",
      " [0.018      0.6666667  0.2857143 ]\n",
      " [0.018      0.6875     0.2857143 ]\n",
      " [0.018      0.7083333  0.2857143 ]\n",
      " [0.018      0.7291667  0.2857143 ]\n",
      " [0.018      0.75       0.2857143 ]\n",
      " [0.017      0.7708333  0.2857143 ]\n",
      " [0.017      0.7916667  0.2857143 ]\n",
      " [0.017      0.8125     0.2857143 ]\n",
      " [0.017      0.8333333  0.2857143 ]\n",
      " [0.018      0.8541667  0.2857143 ]\n",
      " [0.018      0.875      0.2857143 ]\n",
      " [0.018      0.8958333  0.2857143 ]\n",
      " [0.018      0.9166667  0.2857143 ]\n",
      " [0.018      0.9375     0.2857143 ]\n",
      " [0.018      0.9583333  0.2857143 ]\n",
      " [0.018      0.9791667  0.2857143 ]]\n",
      "0.0\n",
      "2398235\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:10:16.885100Z",
     "start_time": "2025-03-30T09:10:16.627579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import pickle\n",
    "\n",
    "# note 这一个cell的目的，是构建cer-e的邻接矩阵\n",
    "\n",
    "# note 首先加载数据\n",
    "count_nodes = 485\n",
    "count_timestamps = 25728\n",
    "dataset_dir = \"/data3/hanzhi/BasicTS/datasets/CER-E\"\n",
    "data = np.memmap(os.path.join(dataset_dir, \"data.dat\"), mode=\"r\", shape=(count_timestamps, count_nodes), dtype=np.float32)\n",
    "print(data.shape)\n",
    "# note 然后计算每一周的任意两个时间序列的相似性\n",
    "period = 48 * 7\n",
    "train_len = int(count_timestamps * 0.7)\n",
    "data = data[:train_len]\n",
    "# 计算标准化\n",
    "sim = np.zeros((data.shape[1], data.shape[1]))\n",
    "tot = 0.\n",
    "for i in tqdm(range(period, len(data), period)):\n",
    "    # (nodes, period)\n",
    "    xi = data[i - period:i].T\n",
    "    # (period, period)\n",
    "    si = np.corrcoef(xi)\n",
    "    sim += si\n",
    "    tot += 1.\n",
    "sim /= tot\n",
    "i = np.arange(count_nodes)\n",
    "sim[i, i] = 0.\n",
    "print(np.mean(sim), np.max(sim), np.min(sim))\n",
    "\n",
    "# note 最后对每一个时间序列，计算与这条时间序列最相似的10条，对于时间序列i，让这十条时间序列adj[i][j]=1，默认情况下，不指向自己\n",
    "# 到时候GWN会自己把它反过来\n",
    "k = 9\n",
    "indices = np.argpartition(sim, -k, axis=1)[:, -k:]\n",
    "# 创建全零的mask数组，并将对应位置标记为True\n",
    "mask = np.zeros_like(sim, dtype=np.float32)\n",
    "np.put_along_axis(mask, indices, True, axis=1)\n",
    "print(np.sum(mask))\n",
    "# 应用mask\n",
    "masked_arr = sim * mask\n",
    "print(np.mean(masked_arr), np.max(masked_arr), np.min(masked_arr))\n",
    "pickle.dump(mask, open(os.path.join(dataset_dir, \"adj_correlation.pkl\"), \"wb\"))"
   ],
   "id": "4c587fb011de469a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25728, 485)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 222.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05183529976004236 0.998338713046769 -0.6031537180581951\n",
      "4365.0\n",
      "0.014066909453355208 0.998338713046769 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cce49dca8182b854"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
